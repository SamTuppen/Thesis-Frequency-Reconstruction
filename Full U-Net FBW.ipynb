{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d597a7b",
   "metadata": {},
   "source": [
    "# U-Net Full implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c3a918",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f63992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from numpy.fft import fft, ifft\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.signal import butter, filtfilt\n",
    "import segyio\n",
    "\n",
    "#TensorFlow libraries\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout, Lambda, Layer, ReLU, Add\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a707ec4",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1659b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters used in gathers\n",
    "Ntrainingsamples = 70     # Number of gathers windowed to generate training examples[-]\n",
    "Ntestsamples = 3          #Number of test examples for the network to interpolate\n",
    "dt = 0.002                 # Sampling rate [s]\n",
    "Fs = 1/dt                  # Sampling frequency (500 Hz)\n",
    "noiselevel = 0.1           # Percent noise\n",
    "RandomFilts = 1            # Turn on for using 2 random 10 Hz frequency ranges\n",
    "TracesPerGather = 636     #Number of traces in each gather. Default for NoSI_short is 636.\n",
    "\n",
    "\n",
    "#Windowing Parameters: (Could merge with the parameters below)\n",
    "input_width = 64         #Columns in window (space)\n",
    "input_height = 512       #Rows in window (time)\n",
    "overlap = 0.5            #Ratio of overlap\n",
    "\n",
    "start_time = 0           #Where to start reading each gather incase you wish to cut off some samples\n",
    "end_time = 1024          #Where to stop reading each gather. Use '-1' to select the last sample\n",
    "\n",
    "\n",
    "#Network parameters\n",
    "dropout = 0.0\n",
    "BatchSize = 4\n",
    "ValidationSplit = 0.2\n",
    "Epochs = 20\n",
    "\n",
    "## File location:\n",
    "TRAINX_PATH = '/Users/samtu/Documents/'\n",
    "filename = 'NoSI_short.segy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086146d7",
   "metadata": {},
   "source": [
    "### Test for GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecdb899",
   "metadata": {},
   "source": [
    "This short line is to test whether your GPU is being used to run the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc0f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.7.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(f'TensorFlow Version:',tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f238a7e",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d696b7",
   "metadata": {},
   "source": [
    "The following cell defines necessary functions to geneate the training and test data, as well as necessary functions to create the neural network.  \n",
    "There are 3 sections:\n",
    "1. Filters\n",
    "2. Loss functions\n",
    "3. Neural network building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7aa7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filters to remove the 'missing' from the broadband data:\n",
    "\n",
    "#Band-stop filter to remove the missing band\n",
    "def myFilterStop(data,flp,fhi,delt):\n",
    "    #c - input time series\n",
    "    #flp - lowpass corner freq of filter\n",
    "    #fhi - highpass corner freq\n",
    "    #delt - sampling interval of the data\n",
    "    \n",
    "    y = data.shape[0]\n",
    "    data2 = np.vstack((np.flipud(data),data,np.flipud(data)))\n",
    "    slope = np.hanning(2*y).reshape((2*y,1))\n",
    "    slope1 = slope[0:y]\n",
    "    slope2 = np.ones((y,1))\n",
    "    slope3 = slope[y:2*y]\n",
    "    slope = np.vstack((slope1, slope2, slope3))\n",
    "\n",
    "    #multiply with a nice window function\n",
    "    for i in range(3*y):\n",
    "        data2[i] = data2[i] * slope[i]\n",
    "\n",
    "    n = 5 #order Butterworth filter\n",
    "    fnq = 1.0 / (2*delt) #Nyquist\n",
    "    Wn = [flp/fnq, fhi/fnq] #Butterworth non-dim freq\n",
    "    b,a = butter(n,Wn,'bandstop',output='ba') #construct the filter\n",
    "    d = filtfilt(b,a,data2,axis=0) #zero phase filter\n",
    "\n",
    "    #pick out the part that I actually want\n",
    "    d = d[1*y + 0:2*y , :]\n",
    "    return d\n",
    "\n",
    "#Band-pass filter to remove all but the missing frequency band. This was used to test the networks perforance on reconstructing\n",
    "#missing frequencty data.\n",
    "def myFilterPass(data,flp,fhi,delt):\n",
    "    #c - input time series\n",
    "    #flp - lowpass corner freq of filter\n",
    "    #fhi - highpass corner freq\n",
    "    #delt - sampling interval of the data\n",
    "    \n",
    "    y = data.shape[0]\n",
    "    data2 = np.vstack((np.flipud(data),data,np.flipud(data)))\n",
    "    slope = np.hanning(2*y).reshape((2*y,1))\n",
    "    slope1 = slope[0:y]\n",
    "    slope2 = np.ones((y,1))\n",
    "    slope3 = slope[y:2*y]\n",
    "    slope = np.vstack((slope1, slope2, slope3))\n",
    "\n",
    "    #multiply with a nice window function\n",
    "    for i in range(3*y):\n",
    "        data2[i] = data2[i] * slope[i]\n",
    "\n",
    "    n = 5 #order Butterworth filter\n",
    "    fnq = 1.0 / (2*delt) #Nyquist\n",
    "    Wn = [flp/fnq, fhi/fnq] #Butterworth non-dim freq\n",
    "    b,a = butter(n,Wn,'bandpass',output='ba') #construct the filter\n",
    "    d = filtfilt(b,a,data2,axis=0) #zero phase filter\n",
    "\n",
    "    #pick out the part that I actually want\n",
    "    d = d[1*y + 0:2*y , :]\n",
    "    return d\n",
    "\n",
    "#Zeroing strong filter\n",
    "def myStrongFilter(data,flp,fhi,delt):\n",
    "    #data - input time series\n",
    "    #flp - lowpass corner freq of filter\n",
    "    #fhi - highpass corner freq\n",
    "    #delt - sampling interval of the data\n",
    "    \n",
    "    flp = flp/2\n",
    "    fhi = fhi/2\n",
    "    \n",
    "    Fs = 1/delt\n",
    "    Nyq = Nyq = Fs/2\n",
    "    \n",
    "    Data_Freq = fft(data,axis=0)\n",
    "    Filter = np.ones(Data_Freq.shape)\n",
    "    Filter[int(np.ceil((flp/Nyq)*Data_Freq.shape[0])):int(np.ceil((fhi/Nyq)*Data_Freq.shape[0])),:] = 0\n",
    "    Filter[-int(np.ceil((fhi/Nyq)*Data_Freq.shape[0])):-int(np.ceil((flp/Nyq)*Data_Freq.shape[0])),:] = 0\n",
    "\n",
    "    Data_Freq_Filt = Data_Freq*Filter\n",
    "    \n",
    "    return np.real(ifft(Data_Freq_Filt,axis=0))\n",
    "\n",
    "\n",
    "#Two custom loss functions:\n",
    "\n",
    "#This loss function looks into the spatially similarity of the gather.\n",
    "def MyLoss(y_true, y_pred):\n",
    "    \n",
    "    #Compute Huber loss\n",
    "    h = tf.keras.losses.Huber()\n",
    "    L1 = h(y_true, y_pred)\n",
    "    \n",
    "    #Compute structural similarity index\n",
    "    SSIM = tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n",
    "    \n",
    "    return   1 - SSIM + L1\n",
    "\n",
    "\n",
    "#Frequency loss function that takes the frequency data into account. \n",
    "def FreqLoss(y_true, y_pred):\n",
    "    \n",
    "    #Create Huber loss function\n",
    "    h = tf.keras.losses.Huber()\n",
    "    L1 = h(y_true, y_pred)\n",
    "    \n",
    "    #Cast in- and output data in complex for to perform fft\n",
    "    y_true_f = tf.dtypes.cast(y_true, tf.complex64)\n",
    "    y_pred_f = tf.dtypes.cast(y_pred, tf.complex64)\n",
    "    \n",
    "    #Compute fft\n",
    "    y_true_fft = tf.signal.fft3d(y_true_f)\n",
    "    y_pred_fft = tf.signal.fft3d(y_pred_f)\n",
    "    \n",
    "    #Compute MAE of frequency domain data\n",
    "    FreqRMS = tf.math.reduce_mean(tf.abs(y_true_fft-y_pred_fft))\n",
    "    \n",
    "    return FreqRMS + L1\n",
    "\n",
    "\n",
    "#Defining necessary layers for the neural network\n",
    "\n",
    "#Coordinate Attention block. Taken from: https://github.com/Andrew-Qibin/CoordAttention\n",
    "#This function is unused in the actual network unless unvommented\n",
    "def CoordAtt(x, reduction=32, bn_trainable=False):\n",
    "    def coord_act(x):\n",
    "        tmpx = (ReLU(max_value=6)(x + 3)) / 6\n",
    "        x = x * tmpx\n",
    "        return x\n",
    "\n",
    "    x_shape = x.shape.as_list()\n",
    "    [b, h, w, c] = x_shape\n",
    "    \n",
    "    x_h = AveragePooling2D(pool_size=(1, w), strides=(1, 1), data_format='channels_last')(x)\n",
    "    x_w = AveragePooling2D(pool_size=(h, 1), strides=(1, 1), data_format='channels_last')(x)\n",
    "    x_w = K.permute_dimensions(x_w, [0, 2, 1, 3])\n",
    "    \n",
    "    y = concatenate(inputs=[x_h, x_w], axis=1)\n",
    "    \n",
    "    mip = max(8, c // reduction)\n",
    "    \n",
    "    y = Conv2D(filters=mip, kernel_size=(1, 1), strides=(1, 1), padding='valid')(y)\n",
    "    y = BatchNormalization(trainable=bn_trainable)(y)\n",
    "    y = coord_act(y)\n",
    "    \n",
    "    x_h, x_w = Lambda(tf.split, arguments={'axis': 1, 'num_or_size_splits': [h, w]})(y)\n",
    "    \n",
    "    x_w = K.permute_dimensions(x_w, [0, 2, 1, 3])\n",
    "    \n",
    "    a_h = Conv2D(filters=c, kernel_size=(1, 1), strides=(1, 1), padding='valid', activation=\"sigmoid\")(x_h)\n",
    "    a_w = Conv2D(filters=c, kernel_size=(1, 1), strides=(1, 1), padding='valid', activation=\"sigmoid\")(x_w)\n",
    "    \n",
    "    out = x * a_h * a_w\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "#Convolution block\n",
    "def conv_block(inp, num_filters):\n",
    "    x = Conv2D(num_filters, (3,3), padding=\"same\", activation=\"relu\",kernel_initializer='HeNormal')(inp)\n",
    "    x = Conv2D(num_filters, (3,3), padding=\"same\", activation=\"relu\",kernel_initializer='HeNormal')(x)\n",
    "#     coordinate_attention = CoordAtt(x)\n",
    "#     x = Add()([coordinate_attention, x])\n",
    "    return x\n",
    "\n",
    "#Encoding block\n",
    "def encoder_block(inp, num_filters):\n",
    "    x = conv_block(inp, num_filters) #also the skip feature\n",
    "    p = MaxPooling2D((2, 2))(x) #output to the next block\n",
    "    return x, p\n",
    "\n",
    "#Decoding block\n",
    "def decoder_block(inp, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (3, 3), strides=(2,2), padding=\"same\",kernel_initializer='HeNormal')(inp)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47511551",
   "metadata": {},
   "source": [
    "## Generate Training data (by windowing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dfbf8a0",
   "metadata": {},
   "source": [
    "This section of the code generates the training gathers for the neural network. The same steps are performed for the training data as for the test data, as follows:  \n",
    "1. Open the `.sgy` or `.segy` file containing the gathers\n",
    "2. Filter the out a certain frequency band with a bandpass filter, which will be the input gathers into the neural network.\n",
    "3. Divide the gather into different (over-lapping) windows, so that there are less parameters being input into the neural network, and thus it is less demanding to run.\n",
    "4. Save the windows containing the filtered gathers into X_train (input) and the unfiltered gathers into Y_train (objective).\n",
    "5. The same steps are repeated for generating the test data.\n",
    "6. Unnecessary windows are removed from the training data.\n",
    "7. The data is normalized by the same coefficient to improve the network's performance. The new scale is from 0 to 1.\n",
    "8. Finally, remaining unused variables are removed to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34cfca8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting training images ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:58<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test images ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting empty arrays ...\n",
      "Deleting duplicate arrays from training data ...\n",
      "Deleting arrays with insignificant SNR ...\n",
      "Normalizing ...\n",
      "Done!\n",
      "There are 4060 training samples.\n",
      "There are 240 test samples.\n"
     ]
    }
   ],
   "source": [
    "# Get and resize train images and masks\n",
    "# Number of layers is num. gather * num. windows per gather\n",
    "\n",
    "#Read the SEGY with segyio and split into gathers\n",
    "counter = 0\n",
    "\n",
    "#Define ouput windows\n",
    "output_width = np.round(overlap*input_width) \n",
    "output_height = np.round(overlap*input_height)\n",
    "        \n",
    "#Open the segy data files.\n",
    "with segyio.open(TRAINX_PATH + filename, ignore_geometry=True) as f:\n",
    "    \n",
    "    print('Getting training images ... ')\n",
    "    NumWindows = int(np.floor(TracesPerGather/output_width)*np.floor(len(f.samples)/output_height))\n",
    "    \n",
    "    #Define matrices to be filledin\n",
    "    X_train = np.zeros((Ntrainingsamples * NumWindows*4, input_height, input_width, 1))\n",
    "    Y_train = np.zeros((Ntrainingsamples * NumWindows*4, input_height, input_width, 1))\n",
    "    \n",
    "    X_test = np.zeros((Ntestsamples * NumWindows*4, input_height, input_width, 1))\n",
    "    Y_test = np.zeros((Ntestsamples * NumWindows*4, input_height, input_width, 1))\n",
    "    \n",
    "    #Frequencies to be removed, note that this number should be changed in case the number/type of filters change\n",
    "    freqRemoveTrain = np.zeros((Ntrainingsamples * NumWindows*4,4))\n",
    "    freqRemoveTest = np.zeros((Ntestsamples * NumWindows*4,4))\n",
    "    \n",
    "    for k in tqdm(np.arange(Ntrainingsamples)):\n",
    "        \n",
    "        #Select the traces of each gather\n",
    "        gather = f.trace.raw[:].T.reshape((len(f.samples),f.tracecount))[start_time:end_time,k+k*(TracesPerGather-1) : k+(k+1)*(TracesPerGather-1)]\n",
    "        \n",
    "        \n",
    "#         #Adding a gain:\n",
    "#         t = np.arange(0,endtime*dt,dt)**1.4\n",
    "#         t2gain = np.repeat(t,numberoftraces).reshape(endtime,numberoftraces)\n",
    "#         gather = gather*t2gain\n",
    "        \n",
    "        \n",
    "        #Filter out frequencies\n",
    "        freqRemoveTrain[k,0] = 30;\n",
    "        freqRemoveTrain[k,1] = freqRemoveTrain[k,0] + 5;\n",
    "\n",
    "        gatherfilt = myStrongFilter(gather,freqRemoveTrain[k,0],freqRemoveTrain[k,1],dt);\n",
    "        \n",
    "        #Apply windowing to gather (My appolgies for making it hard to read, but it's faster than having if/else)\n",
    "        for j in np.arange(0, gather.shape[0], output_height):\n",
    "            for i in np.arange(0, gather.shape[1], output_width):\n",
    "\n",
    "                input_window = gather[int(np.min([np.max([0,j-output_height/2]) , gather.shape[0]-input_height])):\n",
    "                                      int(np.min([np.max([0,j-output_height/2]) + input_height , gather.shape[0]])),\n",
    "                                      int(np.min([np.max([0,i-output_width/2]) , gather.shape[1]-input_width])):\n",
    "                                      int(np.min([np.max([0,i-output_width/2]) + input_width , gather.shape[1]]))]\n",
    "                \n",
    "                input_windowfilt = gatherfilt[int(np.min([np.max([0,j-output_height/2]) , gatherfilt.shape[0]-input_height])):\n",
    "                                              int(np.min([np.max([0,j-output_height/2]) + input_height , gatherfilt.shape[0]])),\n",
    "                                              int(np.min([np.max([0,i-output_width/2]) , gatherfilt.shape[1]-input_width])):\n",
    "                                              int(np.min([np.max([0,i-output_width/2]) + input_width , gatherfilt.shape[1]]))]\n",
    "\n",
    "                #Add noise\n",
    "                input_windowfilt = input_windowfilt + noiselevel * np.random.rand(input_height,input_width)\n",
    "                \n",
    "                #Fill window into training matrix\n",
    "                X_train[counter] = input_windowfilt.reshape(input_height,input_width,1)\n",
    "                Y_train[counter] = input_window.reshape(input_height,input_width,1)\n",
    "    \n",
    "    \n",
    "                #Add to counter\n",
    "                counter += 1\n",
    "            \n",
    "    print('Getting test images ... ')\n",
    "    counter = 0\n",
    "    \n",
    "    gathernormfactor = np.zeros((Ntestsamples * NumWindows*4,1))\n",
    "    gatherfiltnormfactor = np.zeros((Ntestsamples * NumWindows*4,1))\n",
    "    \n",
    "    for k in tqdm(np.arange(Ntrainingsamples, Ntrainingsamples + Ntestsamples)):\n",
    "        \n",
    "        #Select the traces of each gather\n",
    "        gather = f.trace.raw[:].T.reshape((len(f.samples),f.tracecount))[start_time:end_time,k+k*(TracesPerGather-1) : k+(k+1)*(TracesPerGather-1)]\n",
    "        \n",
    "        #Filter out frequencies\n",
    "        freqRemoveTest[k,0] = 30;\n",
    "        freqRemoveTest[k,1] = freqRemoveTest[k,0] + 5;\n",
    "\n",
    "        gatherfilt = myStrongFilter(gather,freqRemoveTest[k,0],freqRemoveTest[k,1],dt);\n",
    "        \n",
    "        #Apply windowing to gather\n",
    "        for j in np.arange(0, gather.shape[0], output_height):\n",
    "            for i in np.arange(0, gather.shape[1], output_width):\n",
    "\n",
    "                input_window = gather[int(np.min([np.max([0,j-output_height/2]) , gather.shape[0]-input_height])):\n",
    "                                      int(np.min([np.max([0,j-output_height/2]) + input_height , gather.shape[0]])),\n",
    "                                      int(np.min([np.max([0,i-output_width/2]) , gather.shape[1]-input_width])):\n",
    "                                      int(np.min([np.max([0,i-output_width/2]) + input_width , gather.shape[1]]))]\n",
    "                \n",
    "                input_windowfilt = gatherfilt[int(np.min([np.max([0,j-output_height/2]) , gather.shape[0]-input_height])):\n",
    "                                              int(np.min([np.max([0,j-output_height/2]) + input_height , gather.shape[0]])),\n",
    "                                              int(np.min([np.max([0,i-output_width/2]) , gather.shape[1]-input_width])):\n",
    "                                              int(np.min([np.max([0,i-output_width/2]) + input_width , gather.shape[1]]))]\n",
    "                \n",
    "                #Add noise\n",
    "                input_windowfilt = input_windowfilt + noiselevel * np.random.rand(input_height,input_width)\n",
    "                \n",
    "                \n",
    "                #Fill window into training matrix\n",
    "                X_test[counter] = input_windowfilt.reshape(input_height,input_width,1)\n",
    "                Y_test[counter] = input_window.reshape(input_height,input_width,1)\n",
    "    \n",
    "    \n",
    "                #Add to counter\n",
    "                counter += 1    \n",
    "    f.close()\n",
    "\n",
    "print('Deleting empty arrays ...')\n",
    "freqRemoveTrain = freqRemoveTrain[~np.all(X_train == 0, axis=(1,2,3))]\n",
    "freqRemoveTest = freqRemoveTest[~np.all(X_test == 0, axis=(1,2,3))]\n",
    "X_train = X_train[~np.all(X_train == 0, axis=(1,2,3))]\n",
    "X_test = X_test[~np.all(Y_test == 0, axis=(1,2,3))]\n",
    "Y_train = Y_train[~np.all(Y_train == 0, axis=(1,2,3))]\n",
    "Y_test = Y_test[~np.all(Y_test == 0, axis=(1,2,3))]\n",
    "\n",
    "\n",
    "print('Deleting duplicate arrays from training data ...')\n",
    "#REMOVE DUPLICATES\n",
    "_, b = np.unique(Y_train, return_index = 1 ,axis=0)\n",
    "X_train = X_train[np.sort(b)] \n",
    "Y_train = Y_train[np.sort(b)]\n",
    "\n",
    "print('Deleting arrays with insignificant SNR ...')\n",
    "#The following windows contain only noisy event prior to the first arrivals.\n",
    "#I need to come up with a better way of removing these (more robust)\n",
    "indexes = np.concatenate([np.arange(0,9,1),np.arange(20,32,1),np.arange(40,57,1),np.arange(60,80,1)])\n",
    "indexes = np.repeat(indexes, Ntrainingsamples, axis=0).reshape((int(indexes.shape[0]),Ntrainingsamples))\n",
    "scalers = np.arange(0,(Ntrainingsamples)*80,80)\n",
    "scalers = np.matlib.repmat(scalers,int(indexes.shape[0]),1)\n",
    "ind2keep = indexes+scalers\n",
    "X_train = X_train[ind2keep.T.flatten()]\n",
    "Y_train = Y_train[ind2keep.T.flatten()]\n",
    "\n",
    "print('Normalizing ...')\n",
    "#Normalization:\n",
    "normfac = np.max(np.array([\n",
    "                              np.max(np.abs(X_train)), \n",
    "                              np.max(np.abs(Y_train)),\n",
    "                              np.max(np.abs(X_test)), \n",
    "                              np.max(np.abs(Y_test))]))\n",
    "X_train = ((X_train / normfac) + 1) / 2\n",
    "Y_train = ((Y_train / normfac) + 1) / 2\n",
    "X_test = ((X_test / normfac) + 1) / 2\n",
    "Y_test = ((Y_test / normfac) + 1) / 2\n",
    "\n",
    "#Deleting unnecessary variables to save memory\n",
    "del gather\n",
    "del _\n",
    "del b\n",
    "del indexes\n",
    "del scalers\n",
    "del ind2keep\n",
    "\n",
    "print('Done!')\n",
    "print(f'There are {X_train.shape[0]} training samples.')\n",
    "print(f'There are {X_test.shape[0]} test samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d9ed9",
   "metadata": {},
   "source": [
    "## Build Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6d9e9",
   "metadata": {},
   "source": [
    "Build the U-Net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53df37d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 512, 64, 32)  320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 512, 64, 32)  9248        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 256, 32, 32)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 256, 32, 64)  18496       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 256, 32, 64)  36928       ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 128, 16, 64)  0          ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 16, 512  295424      ['max_pooling2d_1[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 128, 16, 512  2359808     ['conv2d_4[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 256, 32, 64)  294976     ['conv2d_5[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256, 32, 128  0           ['conv2d_transpose[0][0]',       \n",
      "                                )                                 'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 256, 32, 64)  73792       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 256, 32, 64)  36928       ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 512, 64, 32)  18464      ['conv2d_7[0][0]']               \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512, 64, 64)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 512, 64, 32)  18464       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 512, 64, 32)  9248        ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 512, 64, 1)   33          ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,172,129\n",
      "Trainable params: 3,172,129\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_unet(input_shape, start_neurons):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, start_neurons * 1) \n",
    "    s2, p2 = encoder_block(p1, start_neurons * 2)\n",
    "\n",
    "    b1 = conv_block(p2, start_neurons * 16)\n",
    "\n",
    "    d2 = decoder_block(b1, s2, start_neurons * 2)\n",
    "    d1 = decoder_block(d2, s1, start_neurons * 1)\n",
    "\n",
    "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d1)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model\n",
    "\n",
    "input_shape = (input_height, input_width, 1)\n",
    "model = build_unet(input_shape, 32)\n",
    "model.compile(loss= FreqLoss , optimizer=\"adam\", metrics=[\"mean_squared_error\"],)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18026b49",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992c1ba",
   "metadata": {},
   "source": [
    "This cell will start training the U-Net. The `earlystopper` callback will stop training if no improvement in the loss function has been made for a specified number of epochs. The `checkpointer` callback will save the network's best performing parameters so that the network later be used for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa26a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "earlystopper = EarlyStopping(patience=15, \n",
    "                             verbose=1)\n",
    "checkpointer = ModelCheckpoint('model_unet_checkpoint.h5', \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    Y_train, \n",
    "                    validation_split=ValidationSplit, \n",
    "                    batch_size=BatchSize, \n",
    "                    epochs=Epochs, \n",
    "                    shuffle=1,\n",
    "                    callbacks=[earlystopper, checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0eac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd29c51",
   "metadata": {},
   "source": [
    "## Plotting the progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b76298",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "acc = history.history['mean_squared_error']\n",
    "val_acc = history.history['val_mean_squared_error']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '--b', label='Training error')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation error')\n",
    "plt.title('Training and validation error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ax = plt.figure()\n",
    "plt.rc('font', size=12)\n",
    "plt.plot(epochs, loss, '--b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.locator_params(axis='x', nbins=20)\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0b032",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('model_unet_checkpoint.h5')\n",
    "model = load_model('model_unet_checkpoint.h5', compile=False)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=[\"mean_squared_error\"])\n",
    "\n",
    "preds_test = model.predict(X_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310b6e5",
   "metadata": {},
   "source": [
    "### Prediction test data (unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db160af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform a sanity check on some random validation samples\n",
    "\n",
    "ix = random.randint(0, len(preds_test)-1)\n",
    "# ix = 323\n",
    "print(ix)\n",
    "\n",
    "# for ix in range(len(preds_test)):\n",
    "#     print(ix)\n",
    "#     print(abs(np.array([Y_test[ix,:,:] - preds_test[ix,:,:]])).max())\n",
    "\n",
    "f = plt.figure(figsize=(10,15))\n",
    "ax1 = f.add_subplot(141)\n",
    "plt.imshow(X_test[ix,:,:], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title('Filtered + Noise')\n",
    "plt.clim(0,1)\n",
    "\n",
    "ax2 = f.add_subplot(142)\n",
    "plt.imshow(Y_test[ix,:,:], cmap='gray')\n",
    "plt.title('True data (Objective)')\n",
    "plt.colorbar()\n",
    "plt.clim(0,1)\n",
    "\n",
    "ax3 = f.add_subplot(143)\n",
    "plt.imshow(preds_test[ix,:,:], cmap='gray')\n",
    "plt.title('Fitted data from Neural Network')\n",
    "plt.colorbar()\n",
    "plt.clim(0,1)\n",
    "\n",
    "ax4 = f.add_subplot(144)\n",
    "plt.imshow(abs(Y_test[ix,:,:] - preds_test[ix,:,:]),cmap = 'Reds')\n",
    "plt.title('Absolute error')\n",
    "plt.colorbar()\n",
    "# plt.clim(0,0.05)\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(10,15))\n",
    "ax = f.add_subplot(131)\n",
    "plt.imshow(abs(Y_test[ix,:,:] - preds_test[ix,:,:]),cmap = 'Reds')\n",
    "plt.clim(0, 0.05)\n",
    "plt.title('Absolute error of approximation')\n",
    "\n",
    "ax = f.add_subplot(132)\n",
    "plt.imshow(abs(Y_test[ix,:,:] - X_test[ix,:,:]),cmap = 'Reds')\n",
    "plt.clim(0, 0.05)\n",
    "plt.title('Error Objective and Test Data')\n",
    "\n",
    "\n",
    "ax = f.add_subplot(133)\n",
    "plt.imshow(abs(abs(Y_test[ix,:,:] - X_test[ix,:,:])-abs(Y_test[ix,:,:] - preds_test[ix,:,:])),cmap = 'Reds')\n",
    "plt.title('Difference of both error plots')\n",
    "plt.clim(0, 0.05)\n",
    "plt.show()\n",
    "\n",
    "print('The MAE of the input data was: ' + str(np.abs(np.mean(Y_test[ix,:,:]-X_test[ix,:,:]))))\n",
    "print('The MAE of the ouput data was: ' + str(np.abs(np.mean(Y_test[ix,:,:]-preds_test[ix,:,:]))))\n",
    "print('The max error of the input data was: ' + str(np.max(Y_test[ix,:,:]-X_test[ix,:,:])))\n",
    "print('The max error of the ouput data was: ' + str(np.max(Y_test[ix,:,:]-preds_test[ix,:,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=18)\n",
    "# ix = random.randint(0, len(preds_test)-1)\n",
    "#define matplotlib figure and axis\n",
    "t = np.arange(0,X_test.shape[1]*0.008,0.008)\n",
    "x = np.arange(0,X_test.shape[2]*12.5,12.5)\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "#create simple line plot\n",
    "plt.imshow(X_test[ix,:,:],extent=[0,X_test.shape[2]*12.5,X_test.shape[1]*0.008,0], cmap='gray')\n",
    "plt.clim(0.4, 0.6)\n",
    "plt.xlabel('Offset [m]')\n",
    "plt.ylabel('Time [s]')\n",
    "\n",
    "#set aspect ratio to 1\n",
    "ratio = 3\n",
    "x_left, x_right = ax.get_xlim()\n",
    "y_low, y_high = ax.get_ylim()\n",
    "ax.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "ax.set_xticks((0,250,500,750))\n",
    "\n",
    "#display plot\n",
    "plt.show()\n",
    "\n",
    "#define matplotlib figure and axis\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "#create simple line plot\n",
    "plt.imshow(preds_test[ix,:,:],extent=[0,X_test.shape[2]*12.5,X_test.shape[1]*0.008,0], cmap='gray')\n",
    "plt.clim(0.4, 0.6)\n",
    "plt.xlabel('Offset [m]')\n",
    "plt.ylabel('Time [s]')\n",
    "\n",
    "#set aspect ratio to 1\n",
    "ratio = 3\n",
    "x_left, x_right = ax.get_xlim()\n",
    "y_low, y_high = ax.get_ylim()\n",
    "ax.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "ax.set_xticks((0,250,500,750))\n",
    "\n",
    "#display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0db9d6",
   "metadata": {},
   "source": [
    "# Plotting the frequency and phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271ff7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Frequency Plotting\n",
    "trace = 30\n",
    "windownumber = random.randint(0, len(preds_test)-1)\n",
    "windownumber = 0\n",
    "\n",
    "nfft = 16*len(Y_test[windownumber,:,trace,:])\n",
    "\n",
    "print(f'Showing window: {windownumber}')\n",
    "print(Y_test.shape)\n",
    "print(preds_test.shape)\n",
    "print(freqRemoveTest[windownumber,:])\n",
    "\n",
    "input_gather = ((Y_test[windownumber,:,trace,:].copy()*2)-1)*normfac\n",
    "filtered = ((X_test[windownumber,:,trace,:].copy()*2)-1)*normfac\n",
    "out = (((preds_test[windownumber,:,trace,:].copy())*2)-1)*normfac\n",
    "\n",
    "f1=np.abs(fft(input_gather,axis=0,n=nfft))\n",
    "f2=np.abs(fft(out,axis=0,n=nfft))\n",
    "f3=np.abs(fft(filtered,axis=0,n=nfft))\n",
    "\n",
    "x=np.linspace(0,1/dt,nfft).reshape(nfft,1)\n",
    "\n",
    "p1 = np.angle(fft(input_gather,axis=0,n=nfft))*180/np.pi\n",
    "p2 = np.angle(fft(out,axis=0,n=nfft))*180/np.pi\n",
    "p3 = np.angle(fft(filtered,axis=0,n=nfft))*180/np.pi\n",
    "\n",
    "# idx = (30<=x) * (x<=35)\n",
    "# RMS = np.mean(abs(p1[idx]-p2[idx]))\n",
    "# print(RMS)\n",
    "\n",
    "#Load Filter boundaries\n",
    "fig = plt.figure(figsize=(10,15))\n",
    "fig.add_subplot(3,1,1)\n",
    "plt.rc('font', size=16)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,0], freqRemoveTest[windownumber,1], color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,2], freqRemoveTest[windownumber,3], color='red', alpha=0.2)\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "plt.plot(x,20*np.log10(f1),'r',label='Ground Truth')\n",
    "plt.plot(x,20*np.log10(f2),'--b',label='CNN Output')\n",
    "plt.plot(x,20*np.log10(f3),'--g',color='#00841a',label='Filtered')\n",
    "plt.xlim(20,40)\n",
    "# plt.ylim(140,200)\n",
    "# plt.ylim(-40,30)\n",
    "# plt.title('Frequency response');\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude [dB]')\n",
    "plt.grid()\n",
    "plt.legend(framealpha = 1,fancybox = 0)\n",
    "\n",
    "fig.add_subplot(3,1,2)\n",
    "plt.plot(x,p1,'-r',label='Ground Truth', markerfacecolor='None')\n",
    "plt.plot(x,p2,'--b',label='CNN Output')\n",
    "# plt.plot(x,p3,'--g',label='Filtered')\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,0], freqRemoveTest[windownumber,1], color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,2], freqRemoveTest[windownumber,3], color='red', alpha=0.2)\n",
    "plt.xlim(0,60)\n",
    "# plt.ylim(-np.pi,np.pi)\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Phase [degrees]')\n",
    "# plt.legend(framealpha = 1,fancybox = 0)\n",
    "plt.grid()\n",
    "\n",
    "fig.add_subplot(3,1,3)\n",
    "plt.plot(x,np.abs(np.unwrap(np.angle(fft(input_gather,axis=0,n=nfft)))-np.unwrap(np.angle(fft(out,axis=0,n=nfft))))*180/np.pi,'-or',label='Ground Truth', markerfacecolor='None')\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "plt.xlim(0,60)\n",
    "plt.grid()\n",
    "plt.show\n",
    "\n",
    "f = plt.figure(figsize=(10,15))\n",
    "ax = f.add_subplot(121)\n",
    "plt.imshow(Y_test[windownumber,:,:,:], cmap='gray')\n",
    "plt.title('Objective')\n",
    "plt.clim(0.2,0.8)\n",
    "\n",
    "ax = f.add_subplot(122)\n",
    "plt.imshow(preds_test[windownumber,:,:,:], cmap='gray')\n",
    "plt.title('Prediction')\n",
    "plt.clim(0.2,0.8)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(input_gather,'-r',label='objective')\n",
    "plt.plot(out,'--b',label='prediction')\n",
    "# plt.plot(filtered,'.',label='input data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aad77a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Frequency Plotting\n",
    "trace = 30\n",
    "\n",
    "windownumber = random.randint(0, len(preds_test)-1)\n",
    "windownumber = 60\n",
    "nfft = 16*len(Y_test[windownumber,:,trace,:])\n",
    "\n",
    "\n",
    "print(f'Showing window: {windownumber}')\n",
    "print(Y_test.shape)\n",
    "print(preds_test.shape)\n",
    "print(freqRemoveTest[windownumber,:])\n",
    "\n",
    "input_gather = ((Y_test[windownumber,:,trace,:].copy()*2)-1)*normfac\n",
    "filtered = ((X_test[windownumber,:,trace,:].copy()*2)-1)*normfac\n",
    "out = (((preds_test[windownumber,:,trace,:].copy())*2)-1)*normfac\n",
    "\n",
    "f1=np.abs(fft(input_gather,axis=0,n=nfft))\n",
    "f2=np.abs(fft(out,axis=0,n=nfft))\n",
    "f3=np.abs(fft(filtered,axis=0,n=nfft))\n",
    "\n",
    "x=np.linspace(0,1/dt,nfft).reshape(nfft,1)\n",
    "\n",
    "p1 = np.angle(fft(input_gather,axis=0,n=nfft))*180/np.pi\n",
    "p2 = np.angle(fft(out,axis=0,n=nfft))*180/np.pi\n",
    "p3 = np.angle(fft(filtered,axis=0,n=nfft))*180/np.pi\n",
    "\n",
    "idx = (30<=x) * (x<=35)\n",
    "RMS = np.mean(abs(p1[idx]-p2[idx]))\n",
    "print(RMS)\n",
    "\n",
    "#Load Filter boundaries\n",
    "fig = plt.figure(figsize=(10,15))\n",
    "fig.add_subplot(3,1,1)\n",
    "plt.rc('font', size=16)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,0], freqRemoveTest[windownumber,1], color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,2], freqRemoveTest[windownumber,3], color='red', alpha=0.2)\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "plt.plot(x,20*np.log10(f1),'r',label='Ground Truth')\n",
    "plt.plot(x,20*np.log10(f2),'--b',label='CNN Output')\n",
    "plt.plot(x,20*np.log10(f3),'--g',color='#00841a',label='Filtered')\n",
    "plt.xlim(20,40)\n",
    "# plt.title('Frequency response');\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude [dB]')\n",
    "plt.grid()\n",
    "plt.legend(framealpha = 1,fancybox = 0)\n",
    "\n",
    "fig.add_subplot(3,1,2)\n",
    "plt.plot(x,p1,'-r',label='Ground Truth')#, markerfacecolor='None')\n",
    "plt.plot(x,p2,'--b',label='CNN Output')\n",
    "# plt.plot(x,p3,'--g',label='Filtered')\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,0], freqRemoveTest[windownumber,1], color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,2], freqRemoveTest[windownumber,3], color='red', alpha=0.2)\n",
    "plt.xlim(20,40)\n",
    "# plt.ylim(-np.pi,np.pi)\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Phase [degrees]')\n",
    "# plt.legend(framealpha = 1,fancybox = 0)\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "fig.add_subplot(3,1,3)\n",
    "plt.plot(x,np.abs(np.angle(fft(input_gather,axis=0,n=nfft))-np.angle(fft(out,axis=0,n=nfft)))*180/np.pi,'-or',label='Ground Truth', markerfacecolor='None')\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "plt.hlines(10,0,len(out))\n",
    "plt.xlim(20,40)\n",
    "plt.ylim(0,180)\n",
    "plt.grid()\n",
    "plt.show\n",
    "\n",
    "f = plt.figure(figsize=(10,15))\n",
    "ax = f.add_subplot(131)\n",
    "plt.imshow(((Y_test[windownumber,:,:,:]*2)-1)*normfac, cmap='gray')\n",
    "plt.vlines(trace,0,511,'r')\n",
    "plt.title('Objective')\n",
    "plt.clim(-20,20)\n",
    "\n",
    "ax = f.add_subplot(132)\n",
    "plt.imshow(((preds_test[windownumber,:,:,:]*2)-1)*normfac, cmap='gray')\n",
    "plt.vlines(trace,0,511,'r')\n",
    "plt.title('Prediction')\n",
    "plt.clim(-20,20)\n",
    "\n",
    "ax = f.add_subplot(133)\n",
    "plt.imshow(((X_test[windownumber,:,:,:]*2)-1)*normfac, cmap='gray')\n",
    "plt.vlines(trace,0,511,'r')\n",
    "plt.title('Input')\n",
    "plt.clim(-20,20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(input_gather,'-r',label='objective')\n",
    "plt.plot(out,'--b',label='prediction')\n",
    "# plt.ylim(-50,50)\n",
    "# plt.plot(filtered,'--g',label='input data')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebfdc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Reconstruction of full gather:\n",
    "#So this is the section where we reconstruct the original gather with the computed gathers.\n",
    "trace = 310\n",
    "nfft = 16*len(Y_test[0,:,0,:])\n",
    "\n",
    "gather_obj = np.zeros((end_time,TracesPerGather,1))\n",
    "gather_pred = np.zeros((end_time,TracesPerGather,1))\n",
    "gather_test = np.zeros((end_time,TracesPerGather,1))\n",
    "\n",
    "\n",
    "count = 0\n",
    "for j in np.arange(0, gather_obj.shape[0], output_height):\n",
    "    for i in np.arange(0, gather_obj.shape[1], output_width):\n",
    "        \n",
    "        gather_obj[int(np.min([ np.max([0,j-output_height/2]) , gather_obj.shape[0]-input_height])):\n",
    "             int(np.min([np.max([0,j-output_height/2]) + input_height , gather_obj.shape[0]])),\n",
    "             int(np.min([ np.max([0,i-output_width/2]) , gather_obj.shape[1]-input_width])):\n",
    "             int(np.min([np.max([0,i-output_width/2]) + input_width , gather_obj.shape[1]])),:] = ((Y_test[count,:,:]*2)-1)*normfac\n",
    "        \n",
    "        gather_pred[int(np.min([ np.max([0,j-output_height/2]) , gather_obj.shape[0]-input_height])):\n",
    "             int(np.min([np.max([0,j-output_height/2]) + input_height , gather_obj.shape[0]])),\n",
    "             int(np.min([ np.max([0,i-output_width/2]) , gather_obj.shape[1]-input_width])):\n",
    "             int(np.min([np.max([0,i-output_width/2]) + input_width , gather_obj.shape[1]])),:] = ((preds_test[count,:,:]*2)-1)*normfac\n",
    "        \n",
    "        gather_test[int(np.min([ np.max([0,j-output_height/2]) , gather_obj.shape[0]-input_height])):\n",
    "             int(np.min([np.max([0,j-output_height/2]) + input_height , gather_obj.shape[0]])),\n",
    "             int(np.min([ np.max([0,i-output_width/2]) , gather_obj.shape[1]-input_width])):\n",
    "             int(np.min([np.max([0,i-output_width/2]) + input_width , gather_obj.shape[1]])),:] = ((X_test[count,:,:]*2)-1)*normfac\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "f = plt.figure(figsize=(15,25))\n",
    "ax = f.add_subplot(121)\n",
    "plt.imshow(gather_obj[:,:,0]-gather_test[:,:,0],cmap='gray')\n",
    "plt.clim(-20,20)\n",
    "plt.title('Obj-test')\n",
    "plt.vlines(trace,0,end_time-1,'r')\n",
    "\n",
    "ax = f.add_subplot(122)\n",
    "plt.imshow(gather_obj[:,:,0]-gather_pred[:,:,0],cmap='gray')\n",
    "plt.clim(-20,20)\n",
    "plt.title('Obj-pred')\n",
    "plt.vlines(trace,0,end_time-1,'r')\n",
    "plt.show()\n",
    "\n",
    "x=np.linspace(0,1/dt,nfft).reshape(nfft,1)\n",
    "gather_obj_f = fft(gather_obj,n = nfft,axis=0)\n",
    "gather_pred_f = fft(gather_pred,n = nfft,axis=0)\n",
    "gather_test_f = fft(gather_test,n = nfft,axis=0)\n",
    "\n",
    "gather_obj_phi = np.angle(fft(gather_obj,n = nfft,axis=0))*180/np.pi\n",
    "gather_pred_phi = np.angle(fft(gather_pred,n = nfft,axis=0))*180/np.pi\n",
    "gather_test_phi = np.angle(fft(gather_test,n = nfft,axis=0))*180/np.pi\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "plt.plot(gather_test[:,trace,:],'--g')\n",
    "plt.plot(gather_obj[:,trace,:],'r')\n",
    "plt.plot(gather_pred[:,trace,:],'--b')\n",
    "# plt.ylim()\n",
    "# plt.xlim(0,125)\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "plt.plot(x,abs(gather_test_f[:,trace,:]),'--g')\n",
    "plt.plot(x,abs(gather_obj_f[:,trace,:]),'r')\n",
    "plt.plot(x,abs(gather_pred_f[:,trace,:]),'--b')\n",
    "plt.ylim(0,2000)\n",
    "plt.xlim(20,40)\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "plt.plot(x,(gather_test_phi[:,trace,:]),'--g')\n",
    "plt.plot(x,(gather_obj_phi[:,trace,:]),'r')\n",
    "plt.plot(x,(gather_pred_phi[:,trace,:]),'--b')\n",
    "plt.ylim(-180,180)\n",
    "plt.xlim(20,40)\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "plt.plot(x,np.abs(gather_obj_phi[:,trace,:]-gather_pred_phi[:,trace,:]),'r')\n",
    "plt.ylim(0,180)\n",
    "plt.xlim(20,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_obj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d27e65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### In the above reconstruction we have artifacts from windowing\n",
    "trace = 310\n",
    "nfft = 32*len(Y_test[0,:,0,:])\n",
    "\n",
    "gather_obj = np.zeros((end_time,TracesPerGather,1))\n",
    "gather_pred = np.zeros((end_time,TracesPerGather,1))\n",
    "gather_test = np.zeros((end_time,TracesPerGather,1))\n",
    "\n",
    "level = -output_height\n",
    "location = 0\n",
    "for ind in range(int(len(Y_test[:,0,0,0])/Ntestsamples)):\n",
    "    \n",
    "    if ind < 20:\n",
    "        bound_top = 0\n",
    "        bound_bottom = output_height\n",
    "        \n",
    "    elif ind >= 60:\n",
    "        bound_top = output_height\n",
    "        bound_bottom = input_height\n",
    "    \n",
    "    else:\n",
    "        bound_top = int(output_height/2)\n",
    "        bound_bottom = int((output_height+input_height)/2)\n",
    "    \n",
    "    \n",
    "    if (ind%20) == 0:\n",
    "        bound_left = 0\n",
    "        bound_right = output_width\n",
    "        level += output_height\n",
    "        location = 0\n",
    "        \n",
    "    elif ((ind+1)%20) == 0:\n",
    "        bound_left = output_width\n",
    "        bound_right = input_width\n",
    "        continue\n",
    "    else:\n",
    "        bound_left = int(output_width/2)\n",
    "        bound_right = int((output_width+input_width)/2)\n",
    "        location += output_width\n",
    "#     print(ind)\n",
    "#     plt.imshow(Y_test[ind,int(bound_top):int(bound_bottom),int(bound_left):int(bound_right),:])\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(ind,level,level+output_height,location,location+output_width)\n",
    "    gather_obj[int(level):int(level+output_height),int(location):int(location+output_width)] = ((Y_test[ind,int(bound_top):\n",
    "                                                                                                          int(bound_bottom),\n",
    "                                                                                                          int(bound_left):\n",
    "                                                                                                          int(bound_right),:]*2)-1)*normfac\n",
    "    gather_pred[int(level):int(level+output_height),int(location):int(location+output_width)] = ((preds_test[ind,int(bound_top):\n",
    "                                                                                                          int(bound_bottom),\n",
    "                                                                                                          int(bound_left):\n",
    "                                                                                                          int(bound_right),:]*2)-1)*normfac\n",
    "    gather_test[int(level):int(level+output_height),int(location):int(location+output_width)] = ((X_test[ind,int(bound_top):\n",
    "                                                                                                          int(bound_bottom),\n",
    "                                                                                                          int(bound_left):\n",
    "                                                                                                          int(bound_right),:]*2)-1)*normfac\n",
    "    \n",
    "gather_obj = gather_obj[:,0:int(location+output_width)]\n",
    "gather_pred = gather_pred[:,0:int(location+output_width)]\n",
    "gather_test = gather_test[:,0:int(location+output_width)]\n",
    "\n",
    "time_error = np.sum((gather_obj-gather_pred)**2)\n",
    "time_error2 = np.sum((gather_obj-gather_test)**2)\n",
    "print(f'The overall error of the approximation is: {time_error}')\n",
    "print(f'The overall error of the input data is: {time_error2}')\n",
    "print(f'Or in other words, the error diminished by {(1-(time_error/time_error2))*100} percent')\n",
    "\n",
    "\n",
    "\n",
    "f = plt.figure(figsize=(15,25))    \n",
    "plt.imshow(abs(gather_obj-gather_pred),cmap = 'Reds')\n",
    "plt.vlines(trace,0,end_time-1,'r')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "x=np.linspace(0,1/dt,nfft).reshape(nfft,1)\n",
    "gather_obj_f = fft(gather_obj,n = nfft,axis=0)\n",
    "gather_pred_f = fft(gather_pred,n = nfft,axis=0)\n",
    "gather_test_f = fft(gather_test,n = nfft,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "gather_obj_phi = np.angle(fft(gather_obj,n = nfft,axis=0))*180/np.pi\n",
    "gather_pred_phi = np.angle(fft(gather_pred,n = nfft,axis=0))*180/np.pi\n",
    "gather_test_phi = np.angle(fft(gather_test,n = nfft,axis=0))*180/np.pi\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "plt.plot(gather_test[:,trace,:],'--g')\n",
    "plt.plot(gather_obj[:,trace,:],'r')\n",
    "plt.plot(gather_pred[:,trace,:],'--b')\n",
    "# plt.ylim()\n",
    "# plt.xlim(0,125)\n",
    "plt.show()\n",
    "\n",
    "freq_error = np.sum(np.abs(gather_obj_f-gather_pred_f)**2)\n",
    "freq_error2 = np.sum(np.abs(gather_obj_f-gather_test_f)**2)\n",
    "print(f'The overall error of the approximation is: {freq_error}')\n",
    "print(f'The overall error of the input data is: {freq_error2}')\n",
    "print(f'Or in other words, the error diminished by {(1-(freq_error/freq_error2))*100} percent')\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "plt.plot(x,abs(gather_test_f[:,trace,:]),'--g')\n",
    "plt.plot(x,abs(gather_obj_f[:,trace,:]),'r')\n",
    "plt.plot(x,abs(gather_pred_f[:,trace,:]),'--b')\n",
    "# plt.ylim(0,2000)\n",
    "plt.xlim(20,40)\n",
    "plt.title('Ampltidude')\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "plt.plot(x,(gather_test_phi[:,trace,:]),'--g')\n",
    "plt.plot(x,(gather_obj_phi[:,trace,:]),'r')\n",
    "plt.plot(x,(gather_pred_phi[:,trace,:]),'--b')\n",
    "plt.ylim(-180,180)\n",
    "plt.xlim(20,40)\n",
    "plt.title('Phase')\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "plt.plot(x,np.abs(gather_obj_phi[:,trace,:]-gather_pred_phi[:,trace,:]),'r')\n",
    "plt.ylim(0,180)\n",
    "plt.xlim(20,40)\n",
    "plt.title('Error')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.floor(np.arange(int(len(Y_test[:,0,0,0])/Ntestsamples))/20)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f2573",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Frequency Plotting\n",
    "trace = 30\n",
    "\n",
    "windownumber = random.randint(0, len(preds_test)-1)\n",
    "windownumber = 60\n",
    "nfft = 16*len(Y_test[windownumber,:,trace,:])\n",
    "\n",
    "print(f'Showing window: {windownumber}')\n",
    "print(Y_test.shape)\n",
    "print(preds_test.shape)\n",
    "print(freqRemoveTest[windownumber,:])\n",
    "\n",
    "input_gather = np.zeros((Y_test.shape[1]*2, 1))\n",
    "filtered = np.zeros((Y_test.shape[1]*2, 1))\n",
    "out = np.zeros((Y_test.shape[1]*2, 1))\n",
    "\n",
    "input_gather[0:512] = ((Y_test[0+80,:,trace,:].copy()*2)-1)*normfac\n",
    "input_gather[512:1024] = ((Y_test[60+80,:,trace,:].copy()*2)-1)*normfac\n",
    "\n",
    "filtered[0:512] = ((X_test[0+80,:,trace,:].copy()*2)-1)*normfac\n",
    "filtered[512:1024] = ((X_test[60+80,:,trace,:].copy()*2)-1)*normfac\n",
    "\n",
    "out[0:512] = (((preds_test[0+80,:,trace,:].copy())*2)-1)*normfac\n",
    "out[512:1024] = (((preds_test[60+80,:,trace,:].copy())*2)-1)*normfac\n",
    "\n",
    "f1=np.abs(fft(input_gather,axis=0,n=nfft))\n",
    "f2=np.abs(fft(out,axis=0,n=nfft))\n",
    "f3=np.abs(fft(filtered,axis=0,n=nfft))\n",
    "\n",
    "x=np.linspace(0,1/dt,nfft).reshape(nfft,1)\n",
    "\n",
    "p1 = np.angle(fft(input_gather,axis=0,n=nfft))*180/np.pi\n",
    "p2 = np.angle(fft(out,axis=0,n=nfft))*180/np.pi\n",
    "p3 = np.angle(fft(filtered,axis=0,n=nfft))*180/np.pi\n",
    "\n",
    "idx = (30<=x) * (x<=35)\n",
    "RMS = np.mean(abs(p1[idx]-p2[idx]))\n",
    "print(RMS)\n",
    "\n",
    "#Load Filter boundaries\n",
    "fig = plt.figure(figsize=(10,15))\n",
    "fig.add_subplot(3,1,1)\n",
    "plt.rc('font', size=16)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,0], freqRemoveTest[windownumber,1], color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,2], freqRemoveTest[windownumber,3], color='red', alpha=0.2)\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "plt.plot(x,20*np.log10(f1),'r',label='Ground Truth')\n",
    "plt.plot(x,20*np.log10(f2),'--b',label='CNN Output')\n",
    "plt.plot(x,20*np.log10(f3),'--g',color='#00841a',label='Filtered')\n",
    "plt.xlim(20,40)\n",
    "# plt.title('Frequency response');\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude [dB]')\n",
    "plt.grid()\n",
    "plt.legend(framealpha = 1,fancybox = 0)\n",
    "\n",
    "fig.add_subplot(3,1,2)\n",
    "plt.plot(x,p1,'-r',label='Ground Truth')#, markerfacecolor='None')\n",
    "plt.plot(x,p2,'--b',label='CNN Output')\n",
    "# plt.plot(x,p3,'--g',label='Filtered')\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,0], freqRemoveTest[windownumber,1], color='green', alpha=0.2)\n",
    "# plt.axvspan(freqRemoveTest[windownumber,2], freqRemoveTest[windownumber,3], color='red', alpha=0.2)\n",
    "plt.xlim(20,40)\n",
    "# plt.ylim(-np.pi,np.pi)\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Phase [degrees]')\n",
    "# plt.legend(framealpha = 1,fancybox = 0)\n",
    "plt.grid()\n",
    "\n",
    "fig.add_subplot(3,1,3)\n",
    "plt.plot(x,np.abs(np.angle(fft(input_gather,axis=0,n=nfft))-np.angle(fft(out,axis=0,n=nfft)))*180/np.pi,'-or',label='Ground Truth', markerfacecolor='None')\n",
    "plt.axvspan(30, 35, color='green', alpha=0.2)\n",
    "plt.hlines(10,0,len(out))\n",
    "plt.xlim(20,40)\n",
    "plt.ylim(0,50)\n",
    "plt.grid()\n",
    "plt.show\n",
    "\n",
    "f = plt.figure(figsize=(10,15))\n",
    "ax = f.add_subplot(131)\n",
    "plt.imshow(((Y_test[windownumber,:,:,:]*2)-1)*normfac, cmap='gray')\n",
    "plt.vlines(trace,0,511,'r')\n",
    "plt.title('Objective')\n",
    "plt.clim(-20,20)\n",
    "\n",
    "ax = f.add_subplot(132)\n",
    "plt.imshow(((preds_test[windownumber,:,:,:]*2)-1)*normfac, cmap='gray')\n",
    "plt.vlines(trace,0,511,'r')\n",
    "plt.title('Prediction')\n",
    "plt.clim(-20,20)\n",
    "\n",
    "ax = f.add_subplot(133)\n",
    "plt.imshow(((X_test[windownumber,:,:,:]*2)-1)*normfac, cmap='gray')\n",
    "plt.vlines(trace,0,511,'r')\n",
    "plt.title('Input')\n",
    "plt.clim(-20,20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(input_gather,'-r',label='objective')\n",
    "plt.plot(out,'--b',label='prediction')\n",
    "# plt.ylim(-50,50)\n",
    "# plt.plot(filtered,'--g',label='input data')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f5df0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ix = 20\n",
    "f = plt.figure(figsize=(10,20))\n",
    "ax = f.add_subplot(121)\n",
    "plt.imshow(Y_train[ix,:,:,:]-X_train[ix,:,:,:])\n",
    "\n",
    "ax = f.add_subplot(122)\n",
    "plt.imshow(X_train[ix,:,:,:])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(10,20))\n",
    "ax = f.add_subplot(131)\n",
    "plt.imshow(Y_train[0,:,:,:]-Y_train[1,:,:,:])\n",
    "\n",
    "ax = f.add_subplot(132)\n",
    "plt.imshow(Y_train[0,:,:,:])\n",
    "\n",
    "ax = f.add_subplot(133)\n",
    "plt.imshow(Y_train[0,:,:,:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
